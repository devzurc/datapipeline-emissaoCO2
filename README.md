# ETL Pipeline - AnÃ¡lise de EmissÃµes de CO2

## Sobre o Projeto
Este projeto foi desenvolvido para processar e analisar os dados do conjunto "EmissÃ£o_CO2_por_paÃ­ses.csv". A pipeline ETL segue a arquitetura Medallion (Bronze, Silver, Gold) para garantir eficiÃªncia no armazenamento e processamento dos dados, facilitando anÃ¡lises avanÃ§adas para a equipe de sustentabilidade e gestÃ£o de carbono da empresa.

## Objetivos da MissÃ£o
1. **TransformaÃ§Ã£o dos dados:** Converter os dados para o formato Parquet para otimizar armazenamento e processamento.
2. **Leitura dos arquivos:** Demonstrar como carregar e processar arquivos Parquet.
3. **AnÃ¡lise inicial:** Apresentar a soma de "Kilotons of Co2" por "Region".
4. **Desafio Extra:** Explorar padrÃµes nos dados e identificar regiÃµes com potencial para reduÃ§Ã£o de emissÃµes.

## Arquitetura do Pipeline
```text
ğŸ“‚ data-lakehouse/
 â”œâ”€â”€ ğŸ“‚ bronze/  # Armazena os dados brutos em Parquet
 â”œâ”€â”€ ğŸ“‚ silver/  # Dados processados e limpos
 â”œâ”€â”€ ğŸ“‚ gold/    # Dados prontos para anÃ¡lise de BI
 â”œâ”€â”€ ğŸ“‚ logs/    # Arquivos de log do pipeline
```

## Tecnologias Utilizadas
- **Python** (ETL e anÃ¡lise de dados)
- **Pandas** (ManipulaÃ§Ã£o e limpeza de dados)
- **PyArrow** (ConversÃ£o e processamento Parquet)
- **Logging** (Monitoramento do pipeline)

## ConfiguraÃ§Ã£o do Projeto

### PrÃ©-requisitos
- Python 3.x instalado

## Como Executar o Pipeline Usando o `start.sh`
### 1ï¸âƒ£ Dando PermissÃ£o ao Script `start.sh`

No terminal, navegue atÃ© o diretÃ³rio onde o arquivo `start.sh` estÃ¡ localizado. Suponhamos que o arquivo esteja na pasta **Downloads**:
```sh
cd ~/Downloads
```

Em seguida, torne o script executÃ¡vel com o comando:
```sh
chmod +x start.sh
```

### 2ï¸âƒ£ Executando o Script `start.sh`

Agora, para rodar o script, execute o seguinte comando no terminal:
```sh
./start.sh
```
Ou, caso necessÃ¡rio, execute usando o Bash:
```sh
bash start.sh
```

### 3ï¸âƒ£ O Que o Script `start.sh` Faz

O script `start.sh` realiza os seguintes passos automaticamente:
1. **CriaÃ§Ã£o do ambiente virtual (.venv):** Cria e ativa o ambiente virtual para garantir que todas as dependÃªncias necessÃ¡rias sejam isoladas.
2. **InstalaÃ§Ã£o das dependÃªncias:** Instala todas as dependÃªncias listadas no arquivo `requirements.txt`.
3. **ExecuÃ§Ã£o do ETL:** Roda o pipeline ETL, que realiza as transformaÃ§Ãµes e gera os dados processados nas camadas Bronze, Silver e Gold.
4. **ExecuÃ§Ã£o do Jupyter Notebook:** ApÃ³s a execuÃ§Ã£o do ETL, abre o Jupyter Notebook **`analysis.ipynb`** para que vocÃª possa visualizar e interagir com os dashboards atualizados.

## ExplicaÃ§Ã£o das Camadas

### ğŸŸ¤ Bronze Layer
- Converte o CSV em Parquet e salva na camada Bronze sem modificar os dados.

### âšª Silver Layer
- Processa e limpa os dados:
  - Faz a limpeza e padronizacao dos dados.
  - Ajusta tipos de dados.
  - Renomeia colunas para um padrÃ£o consistente.

### ğŸŸ¡ Gold Layer
- Gera insights para Business Intelligence, como:
  - Total de CO2 emitido por regiÃ£o.
  - PaÃ­ses com maior emissÃ£o de CO2.
  - RegiÃµes com maior potencial de reduÃ§Ã£o de emissÃµes.

## ğŸ“œ Logs e Monitoramento
Os logs do pipeline sÃ£o salvos no diretÃ³rio `logs/etl_pipeline.log`.
